\renewcommand*\chappic{img/megosat.pdf}
\renewcommand*\chapquote{}
\renewcommand*\chapquotesrc{}
\chapter{Results}
\label{ch:results}
%
In Chapter~\ref{ch:sat} we discussed Boolean algebra; in particular we looked at
satisfiability which is practically covered by SAT solvers. SAT solvers take
Boolean functions in Conjunctive Normal Form and determine satisfiability.
In Chapter~\ref{ch:dc} we looked at how we can analyze algorithms by looking
at the progression of differences between algorithm instances. In particular
we looked at hash algorithms introduced in Chapter~\ref{ch:hash}.

With this background we designed an attack setting in Chapter~\ref{ch:enc}
which enables us to verify and also find a hash collision given a differential
characteristic as starting point. Our goal is to find hash collisions
in as little time as possible. Therefore we introduced several performance
tweaks in Section~\ref{sec:enc-algotocnf}.

In this section we will evaluate those approaches. Furthermore we briefly
discuss claims we made about average SAT problems. In Section~\ref{ch:features}
we looked at SAT features which to some extent characterize a SAT problem.

\section{Evaluating SAT features}
\label{sec:results-features}
%
In the introduction of Chapter~\ref{ch:features} we posed 8 questions.
In the following, we want to answer them with the data provided by the
cnf-analysis project.

\begin{description}
\item[Given an arbitrary literal. What is the percentage it is positive?]
  We look at every clause and determine the ratio of positive to the total number of literals.
  We determine the mean per CNF file and the mean among all CNF files
  and retrieve a value of $0.48$ meaning that 48~\% of the literals are positive.
\item[What is the clauses / variables ratio?]
  In average a CNF file has 12,219 variables and 89,541 clauses.
  Its clauses-variables ratio is 7.328.
\item[How many literals occur only once either positive or negative?]
  In average there are 36 existential literals per CNF file,
  but its standard deviation of 967 is very large.
\item[What is the average and longest clause length among CNF benchmarks?]
  The average clause length is 3.04 with a standard deviation of 0.99
  and the longest clause length found was 61,473. Long clauses are typically
  outliers excluding specific assignments.
\item[How many Horn clauses exist in a CNF?]
  In average 29,994 goal clauses and 31,315 definite clauses exist
  with an average number of 83,649 clauses in a CNF file.
\item[Are there any tautological clauses?]
  In a file, 1679 tautological literals have been found. However,
  its mean is 0.07 with a standard deviation of $9.63$ meaning that tautological
  clauses are very rare.
\item[Are there any CNF files with more than one connected variable component?]
  Indeed, an average CNF file contains 67.07 connected variable components.
  However, its median is 1 anyway implying that at least half of the CNF files
  have only 1 connected variable component.
\item[How many variables of a CNF are covered by unit clauses?]
  In average 124 variables are covered by unit clauses. This is an insignificant
  number compared to 12,219 variables in an average CNF.
\end{description}

The clauses/variables ratio was thoroughly studied by the SAT
community~\cite{nudelman2004understanding}.
A strong correlation between the instance's hardness and the ratio of number
of clauses to number variables exists~\cite{selman1996generating}
though it is important to point out that this result holds for randomly
generated SAT instances, which our benchmarks are not classified as.

Existential literals are interesting to discover, because they allow
to remove a clause immediately. Consider a clause with literals
$(l_1, l_2, \ldots, l_n)$. If a guarantee exists such that the variable
of any literal $l_i$ does not occur in any other clause, we can claim
$l_i$ true rendering the clause satisfied.

Tautological clauses trivially also render clauses satisfied.

Connected variable components are interesting, because they split the
SAT problem into several small independent subproblems which can be 
solved in parallel.
Consider two sets of variables $A$ and $B$. Now consider some clauses
using only variables of $A$ and some clauses using only variables of $B$.
The overall CNF is satisfiable iff both clause sets are satisfiable.
The overall CNF is falsifiable iff any clause set is falsifiable.
Hence if we know the connected variable components, we could easily
create two parallel SAT solver instances and solve the problems
independently. 4,607 out of 62,251 CNF files contained more than 1
connected variable component.

These properties represent very fundamental properties of the SAT problem.
But for us the question arises whether we can distinguish our cryptoproblems
from average problems?

\begin{itemize}
\item We looked at 36 files classified as cryptographic problems.
  They are considered cryptographic, because their file or folder name
  indicated they are related to hash functions or general cryptographic
  applications like AES. The specific selection can be identified by
  the crypto tag annotated to these CNF files as part of the cnf-analysis
  project.
\item 62,533 clauses and 8,279 variables in average yield a variables-clauses
  ratio of 7.55.
\item The 36 cryptographic SAT instances give a standard deviation of 0
  for clause length meaning that all clauses had the same length.
\item Whereas the number of connected variable components has a mean
  of 52.73 ($\sigma = 1225$, median $= 1$) for general problems,
  Our cryptographic problems considered had a mean of 1 ($\sigma = 0$, median $= 1$).
\item The number of definite clauses is half its value for general problems
  (16,687 versus 32,787) and the number of goal clauses is 16~\% of its
  value for general problems (5,767 versus 35,094).
\end{itemize}

No other value has been found to be significantly different from
average problems (or its difference follows immediately by the
non-uniform clause length). The number of connected variable
components seems interesting because it might indicate diffusion
in cryptographic problems. Diffusion means that variables strongly
interact with many different variables due to the repetitive
structure of cryptographic primitives. And finally the other
differences can be explained by a certain SAT design which
reoccurs in this benchmarks, because 36 is an exceptionally small
number compared to 62,251 unique CNF problems.

Comparing our average problem with cryptographic problems did
not draw any useful conclusions. Particularly a more thorough discussion
of the SAT designs might be more valuable than our high-level features.
We now specifically look at a SAT design we are familiar with:
Do average SAT problems distinguish from \emph{our} CNF benchmarks?

\begin{itemize}
  \item For all MD4 testcases we have the same number of variables,
    because the internal state of the hash algorithm instances are
    always the same size.
    However, adding the differential description described in
    Section~\ref{sec:enc-diff-desc} increases the number of clauses
    by about 47~\% ($\sigma = 0.0005$) for MD4 instances and
    by about 43~\% ($\sigma = 0.0008$) for SHA-256 instances.
    The additional variable introduced in Section~\ref{sec:enc-diff-desc-eo}
    increases the number of variables by about 80~\%
    and the number of clauses by factor 2.

    \begin{table}[!h]
      \begin{center}
        \begin{tabular}{rc|rc}
          MD4~C & 253,984 / 48,704 & MD4~C diff-desc & 373,920 / 48,704 \\
          MD4~B & 254,210 / 48,704 & MD4~B diff-desc & 374,146 / 48,704 \\
          MD4~A & 254,656 / 48,704 & MD4~A diff-desc & 374,592 / 48,704 \\
          SHA-256 18 & 590,953 / 107,839 & SHA-256 18 diff-desc & 846,487 / 107,839 \\
          SHA-256 21 & 636,838 / 116,800 & SHA-256 21 diff-desc & 911,629 / 116,800 \\
          SHA-256 23 & 667,438 / 122,774 & SHA-256 23 diff-desc & 955,067 / 122,774 \\
          SHA-256 24 & 682,722 / 125,761 & SHA-256 24 diff-desc & 976,770 / 125,761
        \end{tabular}
        \caption{Our testcases and their number of clauses / variables}
      \end{center}
    \end{table}

    Compared to 83,542 clauses and 12,219 variables for our average SAT problem,
    we consider our benchmark to be noticeably large. It is important to
    point out that the problem size does not necessarily correlate with
    the hardness of the SAT problem.

  \item The variables of clauses of average SAT problems
    have a standard deviation of 3,337 in average ($\sigma=$1,261, median $=$3,643).
    Our average SAT problem has a standard deviation of 1,004 in average
    ($\sigma=$13,992, median $=22$). Hence variables which got created at very
    points during the CNF generation are shared within one clause.
    The general statement, that variable enumeration is arbitrary
    and therefore this standard deviation has no meaning, holds but we need
    to consider that practically speaking variables created close to each
    other share close variable indices.
    Under these assumptions a large $\sigma$ indicates variables are reused.
    We assume this is another indicator for high diffusion in cryptographic
    algorithms. Values are intermingled over and over throughout the repetitive
    structure of hash algorithms.

  \item Connected variables components are 129 for MD4 problems and 2 for SHA-256
    problems. We couldn't explain where those components come from and the
    component size would help us to identify their source. We did not investigate
    further, because this number is constant with an increasing problem size.

  \item An average literal frequency of $3.5\cdot 10^{-5}$ for our benchmarks
    is much lower than 0.014 for average problems. We explain this with the
    larger problem size. Literal frequency is divided by the number of clauses
    of the CNF and is therefore smaller the larger the problem is.
\end{itemize}

In general we were not able to identify features which allows us to solve
differential cryptanalysis problems more efficiently compared to
general-purpose SAT problems. We concluded writing your own SAT solver
dedicated to solving differential cryptanalysis problems is not worth
the effort.

\section{Finding hash collisions}
\label{sec:results-attacks}

In this section we look at our benchmark results of Testcases provided in
Appendix~\ref{app:tc}. We make various propositions and substantiate them
with runtime results. Runtimes are always provided in seconds. Therefore
smaller runtimes are better. \timeout{} denotes a timeout (solving took
more than 1 day) and \unknown{} denotes unavailable data.

% We considered MD4 testcases~A, B and C (compare with Appendices~\ref{fig:tcA}, \ref{fig:tcB} and \ref{fig:tcC})
% and generated the corresponding CNF files. The SAT solvers mentioned in Section~\ref{sec:sat-solvers}
% were used to evaluate whether the problem is solvable in reasonably time. For every testcase we
% defined a time limit of at most 1 day (i.e. 86,400 seconds). A timeout is denoted by \timeout.
% Some testcases listed have been evaluated for a larger time limit.

\subsection{Attacking MD4}
\label{sec:results-md4}
%
\begin{prop}
  Testcase~\ref{sec:tcA} in the encoding described in Section~\ref{sec:enc-original}
  can be solved within one minute by all considered SAT solvers.
\end{prop}

In our attack setting we started off with Testcase~\ref{sec:tcA}. It serves
rather as a toy example to verify correctness of our implementation than
as an actual runtime benchmark. Be aware that invalid implementations either
result in unsatisfiability for satisfiable testcases or runtime results are
unexpected because the SAT solver could not take advantage of our SAT design
implementations. This particular testcase can be solved easily with all major
SAT solvers as can be seen in Table~\ref{tab:tcA-results}.

We end up with the result, that the hash collision given in Table~\ref{fig:tcC}
can be solved by a limited set of modern SAT solvers. Of course the cryptanalyst
needs to figure out good starting points for the hash collision and encode them
in the differential characteristic, but this task is still considered practical,
because this task can be easily automated.

\begin{table}[!h]
  \begin{center}
    \begin{tabular}{cccccc}
      \textbf{solver} & \textbf{version} & \textbf{propagations} & \textbf{decisions} & \textbf{restarts} & \textbf{runtime} \\
    \hline
      MiniSat       & 2.2.0   & 3,813,726    & 250,759    & \unknown & 3 \\
      CryptoMiniSat & 4.5.3   & 140,000      & 2,441,566  & 539      & 26 \\
                    & 5       & \unknown     & 2,428,824  & 637      & 37 \\
      Lingeling     & ats1    & 6,586,770    & 436,621    & 980      & 23 \\
      Plingeling    & ats1    & 452,630,440  & 3,275,498  & \unknown & 88 \\
      Treengeling   & ats1    & 271,970,951  & 18,426,013 & \unknown & 1995 \\
      Glucose       & 4.0     & 14,727,839   & 990,491    & 272      & 8 \\
      Glucose Syrup & 4.0     & \unknown     & 629,363    & 201      & 14 \\
    \end{tabular}
    % not listed in folder directory:
    %  treengeling, ats1, algotocnf_1.cnf:
    %   3269623 conflicts, 1639 conflicts per second
    %   18426013 decisions, 9236 decisions per second
    %   271970951 propagations, 0.1 million propagations per second
    %
    %        65   2% lookaheads        26.10 seconds    1%
    %       423  16% splits            98.01 seconds    5%
    %       878  33% simplifications 1633.21 seconds   82%
    %       903  34% searches         237.04 seconds   12%
    %   ======================================================
    %      2627 100% scheduled jobs  1994.94 seconds, 3337 MB
    \caption{Testcase~\ref{sec:tcA} can be solved within 1 minute by all SAT solvers}
    \label{tab:tcA-results}
  \end{center}
\end{table}

\subsection{Evaluating simplification}
\label{sec:results-simplification}
%
As a next approach, we looked at CNF simplifiers. Those simplifiers consume a
CNF file and transform the CNF file to an equisatisfiable CNF file.

\begin{prop}
  Simplification reduces the problem size (number of variables and clauses).
\end{prop}

Consider for example Testcase~\ref{sec:tc18} in the basic encoding introduced
in Section~\ref{sec:enc-original}. Then simplification will reduce the problem
size down by 57--77~\% of its original size (compare with Table~\ref{tab:simpl-size}).
We verified these data for all simplified files and got similar results.
Therefore the proposition holds considering the problem size gets reduced to
approximately half of its size.

\begin{table}[!h]
  \begin{center}
    \begin{tabular}{rcccc}
      \textbf{simplification} & \textbf{variables} &/& \textbf{clauses} & \textbf{(variables left / clauses left)} \\
    \hline
                none &    590953 &/& 107839  & (100~\% / 100~\%) \\
            satelite &    457972 &/& 69670   & (77.50~\% / 64.61~\%) \\
               cmsat &    342139 &/& 61789   & (57.90~\% / 57.30~\%) \\
           lingeling &    344544 &/& 107839  & (58.30~\% / 100.00~\%) \\
             minisat &    391134 &/& 61845   & (66.19~\% / 57.35~\%)
    \end{tabular}
    \caption{
        Problem sizes of Testcase~\ref{sec:tc18} in the encoding of
        Section~\ref{sec:enc-original} after simplification
    }
    \label{tab:simpl-size}
  \end{center}
\end{table}

\begin{prop}
  Simplification as preprocessing step does not significantly improve the runtime of SAT solvers.
\end{prop}
%
We look at Testcase~\ref{sec:tcC} which is a more difficult MD4 problem
compared to Testcase~\ref{sec:tcA}. Simplification runtime results depend on the
SAT solver, which applies certain simplifications while trying to solve the
CNF, and the simplifier used. Table~\ref{tab:simplification-results}
lists runtimes depending on the simplification used.

\begin{description}
\item[none] refers to the unsimplified CNF
\item[cmsat] refers to simplification applied with Cryptominisat version~5: \texttt{./cryptominisat5 -p1 file.cnf simplified.cnf}
\item[lingeling] refers to simplification with lingeling version ats1: \texttt{./lingeling -s file.cnf -o simplified.cnf}
\item[minisat] also simplifies CNF file with the following command line: \texttt{./minisat file.cnf -dimacs=simplified.cnf}
\item[satelite] is specifically designed to simplify CNF files: \texttt{./satelite file.cnf simplified.cnf}
\end{description}

It is worth pointing out that simplification time is not part of the listed
runtime. Simplification can take very long. Especially simplifications with
lingeling have sometimes taken several hours without result.

In conclusion, simplification leads to a slight improvement of the runtime,
but in general we cannot recommend simplifying every CNF file before running it.

\begin{table}[!h]
  \begin{center}
    \begin{tabular}{cc|ccccc}
      \textbf{solver} & \textbf{version} & \textbf{none} & \textbf{cmsat} & \textbf{lingeling} & \textbf{minisat} & \textbf{satelite} \\
    \hline
      MiniSat       & 2.2.0              & 4,519    & 7,649    & \unknown & 1,476    & 1,293 \\
      CryptoMiniSat & 4.5.3              & \unknown & \unknown & \unknown & \unknown & \unknown \\
                    & 5                  & 1,064    & 973      & 1,201    & 4,470    & 3,920 \\
      Lingeling     & ats1               & 1,492    & 906      & 356      & 860      & 1,297 \\
      Treengeling   & ats1               & 1,281    & 13,401   & \unknown & 13,790   & 10,840 \\
      Plingeling    & ats1               & 2,310    & 1,232    & 955      & 1,384    & 2,030 \\
      Glucose       & 4.0                & \unknown & \unknown & \unknown & \unknown & \unknown \\
      Glucose Syrup & 4.0                & \unknown & \unknown & \unknown & \unknown & \unknown
    \end{tabular}
    % algotocnf_1.cnf
    %
    % not listed in runtime results folder contained:
    %    lingeling-ats1:   74.3 seconds, 49.2 MB
    %    treengeling-ats1:  1955 100% scheduled jobs  1281.49 seconds, 2468 MB
    %    cmsat5, ling-simplified 1.cnf:     c Total time               : 1201.19
    %    plingeling, ling-simplified 1.cnf:     c 954.7 seconds, 719.9 MB
    \caption{Runtimes of Testcase~\ref{sec:tcC} after CNF files have been simplified}
    \label{tab:simplification-results}
  \end{center}
\end{table}

\subsection{Improvements with differential description}
\label{sec:result-diff-desc}
%
Our next goal was to scale up to a more difficult problem. We considered SHA-256
which has a much larger internal state (at least by factor 2). So finding a hash
collision is more difficult and we considered further strategies.

\begin{prop}
  A differential description encoding (Section~\ref{sec:enc-diff-desc})
  improves the runtime compared to a missing differential description.
\end{prop}

Consider testcase~C for MD4 (Appendix~\ref{sec:tcA}) and testcases
testcases~18, 21 and 23 (Appendices~\ref{sec:tc18}, \ref{sec:tc21} and
\ref{sec:tc23}) for SHA-256. We evaluate the
runtimes with or without differential description.

Recall that differential description explicitly encodes
how differences in arithmetic and bitwise operations propagate in the CNF.
We discussed \boolf{XOR} and \boolf{MAJ} in Section~\ref{sec:enc-diff-desc}.

The data corresponds to even larger testcases, namely a 27-rounds
variant we looked at, but did not list. This 27-rounds testcase
could not be solved in many cases within our time limit and
is therefore excluded from our lists.

In Table~\ref{tab:diff-desc-results} we randomly picked SAT solvers
and we can clearly see a significant improvement of the runtimes.

We continued by trying the influence the guessing strategy.

\begin{table}[!h]
  \begin{center}
    \begin{tabular}{c|cccc}
                        & \multicolumn{2}{c}{\textbf{CryptoMiniSat~5}} & \multicolumn{2}{c}{\textbf{lingeling-ats1}} \\
      \textbf{testcase} & \textbf{w/o dd} & \textbf{w/ dd} & \textbf{w/o dd} & \textbf{w/ dd} \\
    \hline
      MD4, C            &       1,064 &        231 &      798 &   \unknown \\
      SHA-256, 18       &          37 &         37 &       31 &        160 \\
      SHA-256, 21       &    \unknown &      7,855 &   28,621 &      5,513 \\
      SHA-256, 23       &    \unknown &     26,212 &   76,196 &      1,450
    \end{tabular}
    % not contained in folder structure:
    % SHA256-18, lingeling-ats1, with dd
    %   1805652 decisions, 30042.1 decisions/sec
    %   188760 conflicts, 3140.6 conflicts/sec
    %   47432866 propagations, 0.8 megaprops/sec
    %   60.1 seconds, 79.4 MB
    % MD4-C, lingeling-ats1, w/o dd
    %   18835161 decisions, 12588.8 decisions/sec
    %   9364054 conflicts, 6258.6 conflicts/sec
    %   830017204 propagations, 0.6 megaprops/sec
    %   1496.2 seconds, 147.4 MB
    \caption{Runtimes for various testcases with or without differential description with CryptoMiniSat and lingeling}
    \label{tab:diff-desc-results}
  \end{center}
\end{table}

\subsection{Modifying the guessing strategy}
\label{sec:results-guessing}
%
\newcommand\mone[1][-1]{\texttt{-{}-phase=#1}}
\begin{prop}
  Using \mone{} improves the runtime of lingeling for our testcases.
\end{prop}
%
Option \mone{} of lingeling is described as default phase set to
$-1$ (negative), $0$ (Jeroslow-Wang strategy~\cite{JeroslowWang})
or $1$ (positive). Per default a strategy engineered by
Jeroslow-Wang~\cite{JeroslowWang} is used, but considering
Proposition~\ref{prop:false-first} at page~\pageref{prop:false-first}
we expect \mone{} to provide better runtime results.

Indeed our results consistently indicate a small improvement.
This can be recognized in Table~\ref{tab:phase-results}.

\begin{table}[!h]
  \begin{center}
    \begin{tabular}{c|cc cc cc cc}
      \textbf{testcase} & \multicolumn{2}{c}{18} & \multicolumn{2}{c}{21} & \multicolumn{2}{c}{23} & \multicolumn{2}{c}{24} \\
    \hline
      \textbf{phase}    &       0 &      1 &       0 &      1 &       0 &      1 &       0 &      1 \\
    \hline
      \textbf{runtime}  &      31 &     22 &  28,621 & 19,717 &  76,196 & 71,677 & 85,774  & 70,259 \\
    \end{tabular}
    \caption{lingeling-ats1 results for SHA-256 comparing \mone{} with \mone[0]{}}
    \label{tab:phase-results}
  \end{center}
\end{table}

\subsection{Evaluating the lightweight approach}
\label{sec:lightweight-results}
%
\begin{prop}
  Evaluating difference variables first and with Boolean value false improves the runtime.
\end{prop}
%
The lightweight approach mentioned in Section~\ref{sec:enc-lightweight}
evaluates difference variables first with Boolean value false,
but neglects the differential description.
This approach is justified by the assumption that a low number of differences,
leading to a sparse differential path, is more likely to cancel out differences
ending in a hash collision.

Table~\ref{tab:diff-first-false-results} reveals a nice improvement (the runtime
becomes 0.85~\%) of its original runtime in average.

\begin{table}[!h]
  \begin{center}
    \begin{tabular}{c|c|cccc}
      \textbf{testcase}                   & \textbf{C} & \textbf{18} & \textbf{21} & \textbf{23} & \textbf{24} \\
    \hline
      \textbf{basic approach}      (ats1) &        798 &          31 &      28,621 &      76,196 &      85,774 \\
      \textbf{diff-first-false}  (ats1o1) &        652 &          29 &      27,599 &      59,312 &      66,052
    \end{tabular}
    \caption[Difference variables first (with Boolean value false) results]{
      lingeling-ats1o1 and lingeling-ats1 results
      comparing a difference variables (with Boolean value false) first approach
      with the basic approach
    }
    \label{tab:diff-first-false-results}
  \end{center}
\end{table}

\subsection{Using preference variables}
\label{sec:preference-variables}
%
\begin{prop}
  Adding preference variables lead to a performance improvement.
\end{prop}
%
Section~\ref{sec:enc-diff-desc-eo} introduces preference variables
which enforce the idea that difference variables are evaluated first.
Preference variables only add additional clauses, but do not provide
a runtime improvement per se. The larger number of variables and
clauses make the problem potentially harder.

However, evaluating them with false first makes sure that a low
number of differences is propagated. Otherwise the SAT solver would
spend much time in fruitless branches and the number of restarts
would be comparably high.

Given an assigned difference variable, differential description
ensures that the value is propagated quickly to other parts of
the equation system. This justifies why our encoding with preference
variables should be compared to an instance with differential
description and difference variables first.

Table~\ref{tab:pref-vars-results} shows a significant runtime improvement.

\begin{table}[!h]
  \begin{center}
    \begin{tabular}{c|ccc}
      \textbf{testcase}                                & \textbf{A} &  \textbf{B} &  \textbf{C} \\
    \hline
      \textbf{CNF with differential description}       &         11 &         133 &       155 \\
      \textbf{additionally with preference variables}  &          8 &          50 &        62 \\
    \end{tabular}
    \caption{
      lingeling-ats1o1 testcases comparing an approach
      with differential description with additional preference variables
    }
    \label{tab:pref-vars-results}
  \end{center}
\end{table}

%\subsection{Combined results}
%%
%TODO cmp (basic approach) (diff-first-false with diff-desc)

